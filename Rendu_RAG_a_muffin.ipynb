{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Data engineering"
      ],
      "metadata": {
        "id": "6DjRST934WdE"
      },
      "id": "6DjRST934WdE"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q chromadb sentence-transformers mistralai gradio recipe-scrapers"
      ],
      "metadata": {
        "id": "d02I-_u6Dqyl"
      },
      "id": "d02I-_u6Dqyl",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- IMPORTS  ---\n",
        "import os\n",
        "import json\n",
        "import uuid\n",
        "import time\n",
        "import random\n",
        "from google.colab import files\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from recipe_scrapers import scrape_me\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "import chromadb\n",
        "from mistralai import Mistral\n",
        "\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "KsvopmqijUEP"
      },
      "id": "KsvopmqijUEP",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e34923c2",
      "metadata": {
        "id": "e34923c2"
      },
      "outputs": [],
      "source": [
        "nb_pages = 42  # Nombre de pages Marmiton √† parcourir\n",
        "muffin_dataset = []\n",
        "\n",
        "print(\"D√©but de l'extraction...\")\n",
        "\n",
        "# On parcourt les 42 pages de la recherche \"muffin\" sur Marmiton\n",
        "for page in range(1, nb_pages + 1):\n",
        "    print(f\"Analyse de la page {page}...\")\n",
        "    search_url = f\"https://www.marmiton.org/recettes/recherche.aspx?aqt=muffin&page={page}\"\n",
        "\n",
        "    try:\n",
        "        # Utilisation d'un User-Agent pour simuler un navigateur et √©viter un blocage\n",
        "        response = requests.get(search_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        links = soup.select(\"a[href^='/recettes/recette_']\")\n",
        "\n",
        "        # Nettoyage des URLs de la page actuelle\n",
        "        page_urls = list(set([\"https://www.marmiton.org\" + link['href'] for link in links]))\n",
        "\n",
        "        for url in page_urls:\n",
        "            try:\n",
        "                scraper = scrape_me(url)\n",
        "                # On ne filtre que les recettes dont le titre contient \"muffin\"\n",
        "                if \"muffin\" in scraper.title().lower():\n",
        "                  ing_list = scraper.ingredients()\n",
        "                  ing_string = \", \".join(ing_list)\n",
        "                  # Structuration des donn√©es pour l'indexation future dans ChromaDB\n",
        "                  muffin_dataset.append({\n",
        "                        \"title\": scraper.title(),\n",
        "                        \"ingredients\": ing_string,\n",
        "                        \"instructions\": scraper.instructions(),\n",
        "                        \"time\": scraper.total_time(), # en minutes\n",
        "                        \"yield\": scraper.yields(), # ex: \"12 portions\"\n",
        "                        \"url\": url,\n",
        "                        \"page\" : page\n",
        "                    })\n",
        "                # Pause d'une seconde pour respecter le serveur\n",
        "                time.sleep(1)\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur page {page}: {e}\")\n",
        "\n",
        "# Sauvegarde dans l'espace local de Colab\n",
        "with open('muffins_marmiton_VF.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(muffin_dataset, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"\\n {len(muffin_dataset)} recettes sauvegard√©es dans muffins_marmiton_VF.json\")\n",
        "\n",
        "# T√©l√©chargement automatique\n",
        "files.download('muffins_marmiton_VF.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2, 3) Embedding - Vector store"
      ],
      "metadata": {
        "id": "rvciOJjcd5uM"
      },
      "id": "rvciOJjcd5uM"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1f8bcede",
      "metadata": {
        "id": "1f8bcede"
      },
      "outputs": [],
      "source": [
        "def load_and_prepare_data(json_path):\n",
        "    \"\"\"\n",
        "    Charge le dataset brut et le transforme en dataframe\n",
        "    \"\"\"\n",
        "    with open(json_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    df = pd.DataFrame(data)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e03f25df",
      "metadata": {
        "id": "e03f25df"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_MODEL_NAME = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "COLLECTION_NAME = \"royaume_du_muffin\"\n",
        "\n",
        "def create_embeddings_and_store(df):\n",
        "    \"\"\"\n",
        "    Transforme le texte en embeddings et les stocke dans ChromaDB.\n",
        "    \"\"\"\n",
        "    print(\"ü§ñ Chargement du mod√®le d'embedding...\")\n",
        "    model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
        "\n",
        "    # Concat√©nation Titre + Ingr√©dients pour la recherche\n",
        "    documents = (df['title'] + \" : \" + df['ingredients']).tolist()\n",
        "    # M√©tadonn√©es : On conserve l'int√©gralit√© du DF\n",
        "    metadatas = df.to_dict(orient='records')\n",
        "    ids = [str(uuid.uuid4()) for _ in range(len(df))]\n",
        "\n",
        "    print(\"‚ö° Vectorisation en cours...\")\n",
        "    embeddings = model.encode(documents).tolist()\n",
        "\n",
        "    # Stockage ChromaDB\n",
        "    client = chromadb.Client() # En m√©moire pour le test\n",
        "    try: client.delete_collection(name=COLLECTION_NAME)\n",
        "    except: pass\n",
        "\n",
        "    # Cr√©ation de la collection et indexation des vecteurs\n",
        "    collection = client.create_collection(name=COLLECTION_NAME)\n",
        "    collection.add(documents=documents, embeddings=embeddings, metadatas=metadatas, ids=ids)\n",
        "\n",
        "    print(f\"‚úÖ Indexation termin√©e ! {collection.count()} recettes stock√©es.\")\n",
        "    return collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2be33a73",
      "metadata": {
        "collapsed": true,
        "id": "2be33a73"
      },
      "outputs": [],
      "source": [
        "# 1. Chargement des donn√©es des recettes brutes\n",
        "df = load_and_prepare_data('muffins_marmiton_VF.json')\n",
        "\n",
        "# 2. On cr√©e la base 'db' qui servira au Retrieval (RAG)\n",
        "db = create_embeddings_and_store(df)\n",
        "\n",
        "# 3. Instance locale du mod√®le pour les futures recherches utilisateurs\n",
        "model = SentenceTransformer(EMBEDDING_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4, 5) Retrieval - Generation"
      ],
      "metadata": {
        "id": "LTf3pmmrdsWo"
      },
      "id": "LTf3pmmrdsWo"
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = \"ZYDnduxZSnYxXFC6F7Fyp9tn4Hcoj2SG\"\n",
        "model_mistral = \"mistral-small-latest\"\n",
        "\n",
        "def interroger_chef_muffin(query_str):\n",
        "    # Question de l'utilisateur vectoris√©e\n",
        "    query_vector = model.encode([query_str]).tolist()\n",
        "\n",
        "    # 1. RETRIEVAL : On cherche les 5 meilleures recettes dans la base ChromaDB\n",
        "    search_results = db.query(query_embeddings=query_vector, n_results=5)\n",
        "    recettes = search_results['metadatas'][0]\n",
        "\n",
        "    # Podium affich√© pour mes tests en interne, pas pour l'utilisateur externe\n",
        "    podium_html = \"### Podium des Recettes (Sources)\\n\"\n",
        "    context_str = \"\"\n",
        "    for i, r in enumerate(recettes):\n",
        "        infos = f\"**{i+1}. {r['title']}**\\n- üîó [Lien vers la recette]({r['url']})\\n\\n\"\n",
        "        podium_html += infos\n",
        "        # On garde le format complet pour le contexte envoy√© au LLM\n",
        "        context_str += f\"--- {r['title']} ---\\n{r['time']}\\n{r['yield']}\\n{r['ingredients']}\\n{r['instructions']}\\n{r['url']}\\n\"\n",
        "\n",
        "    # 2. GENERATION\n",
        "    prompt_final = f\"\"\"\n",
        "    TU ES \"CHEF MUFFIN\". TON UNIQUE MISSION EST DE DONNER DES RECETTES COMPL√àTES DE MUFFINS, RIEN D'AUTRE. R√©ponds toujours avec humour et gentillesse : tu es un jeune chef dr√¥le, avec un c√¥t√© Reggaeman !\n",
        "\n",
        "    ### R√àGLE D'OR :\n",
        "    NE DEMANDE JAMAIS √† l'utilisateur s'il veut les d√©tails. DONNE-LES TOUT DE SUITE.\n",
        "    L'utilisateur ne peut pas te r√©pondre, c'est ta SEULE chance de l'aider !\n",
        "\n",
        "    ### DIRECTIVES STRICTES :\n",
        "    1. S√âLECTION : Soit l'utilisateur demande des ingr√©dients qui correspondent tr√®s bien √† une recette du [CONTEXTE] en particulier, dans ce cas ne propose QUE CELLE-CI.\n",
        "    Soit les recettes du [CONTEXTE] sont similaires mais ne correspondent pas parfaitement √† la demande, propose les TOUTES, SEULEMENT SI ELLES CONTIENNENT AU MOINS UN INGREDIENT DE LA DEMANDE, ou si un ingr√©dient est de la bonne famille d'aliments .\n",
        "    Si la [QUESTION] contient des objets non comestibles ou des ingr√©dients farfelus, ne cherche pas √† les inclure. R√©ponds avec humour que Chef Muffin ne cuisine pas de [OBJET] et propose tes meilleures recettes sucr√©es √† la place.\n",
        "\n",
        "    2. FORMAT COMPLET : Pour chaque recette choisie, tu DOIS utiliser ce format pr√©cis :\n",
        "       - ### [Emoji] [Titre exact]\n",
        "       - **‚è± Dur√©e :** [Dur√©e] | **üßÅ Portions :** [Nombre]\n",
        "\n",
        "       **Ingr√©dients :**\n",
        "       [Liste des ingr√©dients avec des tirets]\n",
        "\n",
        "       **Instructions :**\n",
        "       [Liste num√©rot√©e des √©tapes]\n",
        "\n",
        "       üåê Source : [URL]\n",
        "    3. ANCRAGE : Si l'ingr√©dient pr√©cis n'existe pas dans le contexte, dis honn√™tement que tu ne l'as pas en stock et DONNE LA RECETTE ENTI√àRE imm√©diatement apr√®s.\n",
        "    4. PAS DE BLA-BLA : √âvite les listes de titres inutiles. Si tu cites une recette, tu donnes ses instructions.\n",
        "    5. LANGUE : R√©ponds toujours en fran√ßais courant et app√©tissant. Ne fais pas que donner la recette, tu peux la pr√©senter, dire si elle correspond aux ingr√©dients demand√©s ou pas.\n",
        "    6. Utilise UNIQUEMENT les recettes fournies dans le bloc [CONTEXTE]. N'invente rien et NE MODIFIE JAMAIS une recette.\n",
        "\n",
        "\n",
        "    [CONTEXTE]\n",
        "    {context_str}\n",
        "\n",
        "    [QUESTION]\n",
        "    {query_str}\n",
        "    \"\"\"\n",
        "\n",
        "    # 3. APPEL √Ä MISTRAL\n",
        "    client = Mistral(api_key=api_key)\n",
        "\n",
        "    response = client.chat.complete(\n",
        "        model=model_mistral,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt_final}]\n",
        "    )\n",
        "\n",
        "    reponse_llm = response.choices[0].message.content\n",
        "\n",
        "    return reponse_llm\n"
      ],
      "metadata": {
        "id": "ZLyxLL8zEupT"
      },
      "id": "ZLyxLL8zEupT",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo = gr.Interface(\n",
        "    fn=interroger_chef_muffin,\n",
        "    inputs=gr.Textbox(label=\"üåø Qu'est ce que tu veux dans ton muffin, man ?\"),\n",
        "    outputs=gr.Markdown(label=\"üë®‚Äçüç≥ La parole du Chef Winston\"),\n",
        "    title=\"Bienvenue dans l'atelier du Chef Winston Muffin ! üáØüá≤ \",\n",
        "    description=\"Le Chef √©toil√© qui ne jure que par les muffins üßÅ\",\n",
        "    theme=\"ocean\",\n",
        "    allow_flagging=\"never\"\n",
        ")\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "id": "5O4EuneVqBky"
      },
      "id": "5O4EuneVqBky",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Alternative : on enrichit les embeddings en identifiant les identifiants principaux de chaque recette"
      ],
      "metadata": {
        "id": "Ri6dINj4dbOc"
      },
      "id": "Ri6dINj4dbOc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vous pouvez ex√©cuter toutes ces cellules √† la suite pour obtenir la version alternative."
      ],
      "metadata": {
        "id": "ve-RSvAPrqdV"
      },
      "id": "ve-RSvAPrqdV"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CONFIGURATION ---\n",
        "API_KEY = \"ZYDnduxZSnYxXFC6F7Fyp9tn4Hcoj2SG\"\n",
        "EMBEDDING_MODEL_NAME = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "COLLECTION_NAME = \"royaume_du_muffin\"\n",
        "\n",
        "client = Mistral(api_key=API_KEY)\n",
        "model_embedding = SentenceTransformer(EMBEDDING_MODEL_NAME)"
      ],
      "metadata": {
        "id": "Wd3N0KpJGEyD"
      },
      "id": "Wd3N0KpJGEyD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fonction ***extraire_stars_avec_mistral_robuste*** utilis√©e pour le pr√©-traitement des donn√©es : identifie les ingr√©dients principaux gr√¢ce √† Mistral pour am√©liorer la performance du RAG."
      ],
      "metadata": {
        "id": "sfghTsSsKMhe"
      },
      "id": "sfghTsSsKMhe"
    },
    {
      "cell_type": "code",
      "source": [
        "def extraire_stars_avec_mistral_robuste(row, retries=5):\n",
        "    prompt = f\"\"\"\n",
        "    Analyse cette recette et extrais UNIQUEMENT les 2 ou 3 ingr√©dients signatures.\n",
        "    Titre : {row['title']}\n",
        "    Ingr√©dients : {row['ingredients']}\n",
        "    R√©ponse (mots-cl√©s s√©par√©s par une virgule uniquement) :\"\"\"\n",
        "\n",
        "    for i in range(retries):\n",
        "        try:\n",
        "            response = client.chat.complete(\n",
        "                model=\"mistral-tiny\", # Mod√®le rapide pour l'extraction de mots-cl√©s\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "            )\n",
        "            # Pause pour stabiliser le d√©bit de requ√™tes\n",
        "            time.sleep(0.5)\n",
        "            # Nettoyage de la r√©ponse pour garantir une uniformit√© dans la base de donn√©es\n",
        "            return response.choices[0].message.content.strip().lower()\n",
        "\n",
        "        except Exception as e:\n",
        "            if \"429\" in str(e):\n",
        "                # Strat√©gie de 'Backoff Exponentiel' : on attend\n",
        "                # (2^i) + un facteur al√©atoire pour √©viter que toutes les requ√™tes ne repartent exactement au m√™me moment\n",
        "                attente = (2 ** i) + random.random()\n",
        "                print(f\" Trop rapide ! Pause de {attente:.1f}s...\")\n",
        "                time.sleep(attente)\n",
        "            else:\n",
        "                print(f\"‚ùå Erreur sur '{row['title']}': {e}\")\n",
        "                return \"\"\n",
        "    # Si apr√®s 5 tentatives rien ne fonctionne, on renvoie une cha√Æne vide pour ne pas bloquer le script\n",
        "    return \"\""
      ],
      "metadata": {
        "id": "c1tbD3D4Jj6s"
      },
      "id": "c1tbD3D4Jj6s",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fonction ***enrichir_batch*** pour g√©n√©rer le dataset final \"enrichi\" : je l'ai fait par groupes de 100 recettes pour ne pas perdre les donn√©es d√©j√† obtenues en cas de bug. Les ingr√©dients principaux sont r√©p√©t√©s 3 fois dans l'embedding"
      ],
      "metadata": {
        "id": "NwKgkoMpKqwA"
      },
      "id": "NwKgkoMpKqwA"
    },
    {
      "cell_type": "code",
      "source": [
        "def enrichir_batch(df, batch_size=100, output_file=\"muffins_enriched_dataset.json\"):\n",
        "    df = df.copy()\n",
        "    total_recettes = len(df)\n",
        "\n",
        "    # On initialise la colonne 'ingr√©dients stars' (principaux) si elle n'existe pas\n",
        "    if 'star_ingredients' not in df.columns:\n",
        "        df['star_ingredients'] = \"\"\n",
        "\n",
        "    print(f\"Lancement du traitement de {total_recettes} muffins par blocs de {batch_size}...\")\n",
        "\n",
        "    for start_idx in range(0, total_recettes, batch_size):\n",
        "        end_idx = min(start_idx + batch_size, total_recettes)\n",
        "        print(f\"\\n Traitement du bloc : {start_idx} √† {end_idx}...\")\n",
        "\n",
        "        # On ne traite que les lignes du bloc\n",
        "        for idx in tqdm(range(start_idx, end_idx)):\n",
        "            # On v√©rifie si on n'a pas d√©j√† l'info (pour pouvoir relancer le script apr√®s un crash)\n",
        "            if not df.loc[idx, 'star_ingredients']:\n",
        "                df.loc[idx, 'star_ingredients'] = extraire_stars_avec_mistral_robuste(df.iloc[idx])\n",
        "\n",
        "        # Sauvegarde interm√©diaire apr√®s chaque bloc\n",
        "        df.to_json(output_file, orient='records', force_ascii=False, indent=4)\n",
        "        print(f\"‚úÖ Bloc termin√© et sauvegard√© dans {output_file}\")\n",
        "\n",
        "    # Une fois fini, on cr√©e la colonne finale pour l'embedding\n",
        "    df['text_for_embedding'] = df.apply(\n",
        "        lambda x: f\"{x['star_ingredients']} {x['star_ingredients']} {x['star_ingredients']} {x['title']} : {x['ingredients']}\",\n",
        "        axis=1\n",
        "        )\n",
        "\n",
        "    # Sauvegarde finale\n",
        "    df.to_json(output_file, orient='records', force_ascii=False, indent=4)\n",
        "    print(f\"\\n Traitement termin√© ! Fichier sauvegard√© : {output_file}\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "tI0lYoI7gAas"
      },
      "id": "tI0lYoI7gAas",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chargement du dataset optimis√© comme pour la version standard\n"
      ],
      "metadata": {
        "id": "7av6dSSpNWCt"
      },
      "id": "7av6dSSpNWCt"
    },
    {
      "cell_type": "code",
      "source": [
        "def create_embeddings_and_store_optimized(df):\n",
        "    print(f\"ü§ñ Initialisation de ChromaDB...\")\n",
        "\n",
        "    # On vectorise la colonne 'text_for_embedding' (d√©j√† pr√©par√©e dans le JSON)\n",
        "    documents = df['text_for_embedding'].astype(str).tolist()\n",
        "    metadatas = df.to_dict(orient='records')\n",
        "    ids = [str(uuid.uuid4()) for _ in range(len(df))]\n",
        "\n",
        "    embeddings = model_embedding.encode(documents, show_progress_bar=True).tolist()\n",
        "\n",
        "    client_db = chromadb.Client()\n",
        "    try: client_db.delete_collection(name=COLLECTION_NAME)\n",
        "    except: pass\n",
        "\n",
        "    collection = client_db.create_collection(\n",
        "        name=COLLECTION_NAME,\n",
        "        metadata={\"hnsw:space\": \"cosine\"}\n",
        "    )\n",
        "\n",
        "    collection.add(documents=documents, embeddings=embeddings, metadatas=metadatas, ids=ids)\n",
        "    print(f\"‚úÖ Base vectorielle pr√™te : {collection.count()} muffins index√©s.\")\n",
        "    return collection"
      ],
      "metadata": {
        "id": "cTyJv-v5LH2G"
      },
      "id": "cTyJv-v5LH2G",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# On charge directement le fichier pr√©par√©\n",
        "try:\n",
        "    df_muffins = pd.read_json(\"muffins_enriched_dataset.json\")\n",
        "    print(\"‚úÖ Grimoire charg√© avec succ√®s !\")\n",
        "except:\n",
        "    print(\"‚ùå Erreur : le fichier 'muffins_enriched_dataset.json' est introuvable.\")\n",
        "\n",
        "# --- CR√âATION DE LA DB VECTORIELLE ---\n",
        "db = create_embeddings_and_store_optimized(df_muffins)"
      ],
      "metadata": {
        "id": "OJ-YuLvxNTtY"
      },
      "id": "OJ-YuLvxNTtY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = \"ZYDnduxZSnYxXFC6F7Fyp9tn4Hcoj2SG\"\n",
        "model_mistral = \"mistral-small-latest\"\n",
        "\n",
        "def interroger_chef_muffin(query_str):\n",
        "    # Question de l'utilisateur vectoris√©e\n",
        "    query_vector = model_embedding.encode([query_str]).tolist()\n",
        "\n",
        "    # 1. RETRIEVAL : On cherche les 5 meilleures recettes dans la base ChromaDB\n",
        "    search_results = db.query(query_embeddings=query_vector, n_results=5)\n",
        "    recettes = search_results['metadatas'][0]\n",
        "\n",
        "    # Podium affich√© pour mes tests en interne, pas pour l'utilisateur externe\n",
        "    podium_html = \"### Podium des Recettes (Sources)\\n\"\n",
        "    context_str = \"\"\n",
        "    for i, r in enumerate(recettes):\n",
        "        infos = f\"**{i+1}. {r['title']}**\\n- üîó [Lien vers la recette]({r['url']})\\n\\n\"\n",
        "        podium_html += infos\n",
        "        # On garde le format complet pour le contexte envoy√© au LLM\n",
        "        context_str += f\"--- {r['title']} ---\\n{r['time']}\\n{r['yield']}\\n{r['ingredients']}\\n{r['instructions']}\\n{r['url']}\\n\"\n",
        "\n",
        "    # 2. GENERATION\n",
        "    prompt_final = f\"\"\"\n",
        "    TU ES \"CHEF MUFFIN\". TON UNIQUE MISSION EST DE DONNER DES RECETTES COMPL√àTES DE MUFFINS, RIEN D'AUTRE. R√©ponds toujours avec humour et gentillesse : tu es un jeune chef dr√¥le, avec un c√¥t√© Reggaeman !\n",
        "\n",
        "    ### R√àGLE D'OR :\n",
        "    NE DEMANDE JAMAIS √† l'utilisateur s'il veut les d√©tails. DONNE-LES TOUT DE SUITE.\n",
        "    L'utilisateur ne peut pas te r√©pondre, c'est ta SEULE chance de l'aider !\n",
        "\n",
        "    ### DIRECTIVES STRICTES :\n",
        "    1. S√âLECTION : Soit l'utilisateur demande des ingr√©dients qui correspondent tr√®s bien √† une recette du [CONTEXTE] en particulier, dans ce cas ne propose QUE CELLE-CI.\n",
        "    Soit les recettes du [CONTEXTE] sont similaires mais ne correspondent pas parfaitement √† la demande, propose les TOUTES, SEULEMENT SI ELLES CONTIENNENT AU MOINS UN INGREDIENT DE LA DEMANDE, ou si un ingr√©dient est de la bonne famille d'aliments .\n",
        "    Si la [QUESTION] contient des objets non comestibles ou des ingr√©dients farfelus, ne cherche pas √† les inclure. R√©ponds avec humour que Chef Muffin ne cuisine pas de [OBJET] et propose tes meilleures recettes sucr√©es √† la place.\n",
        "\n",
        "    2. FORMAT COMPLET : Pour chaque recette choisie, tu DOIS utiliser ce format pr√©cis :\n",
        "       - ### [Emoji] [Titre exact]\n",
        "       - **‚è± Dur√©e :** [Dur√©e] | **üßÅ Portions :** [Nombre]\n",
        "\n",
        "       **Ingr√©dients :**\n",
        "       [Liste des ingr√©dients avec des tirets]\n",
        "\n",
        "       **Instructions :**\n",
        "       [Liste num√©rot√©e des √©tapes]\n",
        "\n",
        "       üåê Source : [URL]\n",
        "    3. ANCRAGE : Si l'ingr√©dient pr√©cis n'existe pas dans le contexte, dis honn√™tement que tu ne l'as pas en stock et DONNE LA RECETTE ENTI√àRE imm√©diatement apr√®s.\n",
        "    4. PAS DE BLA-BLA : √âvite les listes de titres inutiles. Si tu cites une recette, tu donnes ses instructions.\n",
        "    5. LANGUE : R√©ponds toujours en fran√ßais courant et app√©tissant. Ne fais pas que donner la recette, tu peux la pr√©senter, dire si elle correspond aux ingr√©dients demand√©s ou pas.\n",
        "    6. Utilise UNIQUEMENT les recettes fournies dans le bloc [CONTEXTE]. N'invente rien et NE MODIFIE JAMAIS une recette.\n",
        "\n",
        "\n",
        "    [CONTEXTE]\n",
        "    {context_str}\n",
        "\n",
        "    [QUESTION]\n",
        "    {query_str}\n",
        "    \"\"\"\n",
        "\n",
        "    # 3. APPEL √Ä MISTRAL\n",
        "    client = Mistral(api_key=api_key)\n",
        "\n",
        "    response = client.chat.complete(\n",
        "        model=model_mistral,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt_final}]\n",
        "    )\n",
        "\n",
        "    reponse_llm = response.choices[0].message.content\n",
        "\n",
        "    return reponse_llm\n"
      ],
      "metadata": {
        "id": "y4azdfW7Yp53"
      },
      "execution_count": 12,
      "outputs": [],
      "id": "y4azdfW7Yp53"
    },
    {
      "cell_type": "code",
      "source": [
        "demo = gr.Interface(\n",
        "    fn=interroger_chef_muffin,\n",
        "    inputs=gr.Textbox(label=\"üåø Qu'est ce que tu veux dans ton muffin, man ? \"),\n",
        "    outputs=gr.Markdown(label=\"üë®‚Äçüç≥ La parole du Chef Winston\"),\n",
        "    title=\"Bienvenue dans l'atelier du Chef Winston Muffin ! üáØüá≤ \",\n",
        "    description=\"Le Chef √©toil√© qui ne jure que par les muffins üßÅ\",\n",
        "    theme=\"ocean\",\n",
        "    allow_flagging=\"never\"\n",
        ")\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "id": "b0t6sySIOBlo"
      },
      "execution_count": null,
      "outputs": [],
      "id": "b0t6sySIOBlo"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "notebooks (3.9.12)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
